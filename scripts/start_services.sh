#!/bin/bash

echo "ğŸš€ Starting LLM Inference Stress Testing Services..."
echo "=================================================="

# Start Ollama service
echo "ğŸ¦™ Starting Ollama service..."
brew services start ollama
if [ $? -eq 0 ]; then
    echo "âœ… Ollama service started"
    # Wait for Ollama to be ready
    echo "â³ Waiting for Ollama to be ready..."
    sleep 3
    
    # Test if Ollama is responding
    if curl -s http://localhost:11434 > /dev/null; then
        echo "âœ… Ollama is responding on localhost:11434"
    else
        echo "âš ï¸  Ollama may need more time to start"
    fi
else
    echo "âŒ Failed to start Ollama service"
fi

# List available models
echo ""
echo "ğŸ“‹ Available Ollama models:"
ollama list 2>/dev/null || echo "âŒ Ollama not ready yet"

# Start Streamlit dashboard
echo ""
echo "ğŸ“± Starting Streamlit dashboard..."
echo "ğŸŒ Dashboard will be available at: http://localhost:8501"
echo "ğŸ“Š To start the dashboard, run:"
echo "   streamlit run app.py"

echo "=================================================="
echo "ğŸ‰ Services are ready!"
echo ""
echo "ğŸš€ Next Steps:"
echo "1. Activate virtual environment:"
echo "   source .venv-llm-infer/bin/activate"
echo ""
echo "2. Start Streamlit dashboard:"
echo "   streamlit run app.py --server.runOnSave true"
echo ""
echo "3. Open browser: http://localhost:8501"
echo ""
echo "ğŸ’¡ Or use the automated development script:"
echo "   ./dev_start.sh"
echo ""
echo "ğŸ›‘ To shut down everything later, run:"
echo "   ./shutdown_services.sh" 