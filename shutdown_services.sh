#!/bin/bash

echo "ğŸ›‘ Shutting down LLM Inference Stress Testing Services..."
echo "=================================================="

# Stop all Streamlit processes
echo "ğŸ“± Stopping Streamlit processes..."
pkill -f "streamlit run"
if [ $? -eq 0 ]; then
    echo "âœ… Streamlit processes stopped"
else
    echo "â„¹ï¸  No Streamlit processes found"
fi

# Stop Ollama service (if running as brew service)
echo "ğŸ¦™ Stopping Ollama service..."
brew services stop ollama 2>/dev/null
if [ $? -eq 0 ]; then
    echo "âœ… Ollama brew service stopped"
else
    echo "â„¹ï¸  Ollama brew service was not running"
fi

# Kill any remaining Ollama processes
echo "ğŸ”ª Killing any remaining Ollama processes..."
pkill -f "ollama"
if [ $? -eq 0 ]; then
    echo "âœ… Ollama processes killed"
else
    echo "â„¹ï¸  No Ollama processes found"
fi

# Optional: Free up GPU memory (if you were using GPU)
echo "ğŸ§¹ Clearing any cached models from memory..."
# This is more relevant for CUDA, but good practice
python3 -c "import gc; gc.collect()" 2>/dev/null

echo "=================================================="
echo "ğŸ‰ All LLM services have been shut down!"
echo "ğŸ’¾ Model files are still cached locally for faster startup next time"
echo "ğŸ”‹ Your system is now free to shut down safely"

# Check if any processes are still running
echo ""
echo "ğŸ” Checking for any remaining processes..."
REMAINING=$(ps aux | grep -E "(streamlit|ollama)" | grep -v grep | wc -l)
if [ $REMAINING -eq 0 ]; then
    echo "âœ… Clean shutdown - no processes remaining"
else
    echo "âš ï¸  Warning: $REMAINING processes may still be running:"
    ps aux | grep -E "(streamlit|ollama)" | grep -v grep
fi 