#!/bin/bash

echo "ğŸ›‘ Shutting down LLM Inference Stress Testing Services..."
echo "=================================================="

# Stop all Streamlit processes
echo "ğŸ“± Stopping Streamlit processes..."
pkill -f "streamlit run"
if [ $? -eq 0 ]; then
    echo "âœ… Streamlit processes stopped"
else
    echo "â„¹ï¸  No Streamlit processes found"
fi

# Stop Ollama service (if running as brew service)
echo "ğŸ¦™ Stopping Ollama service..."
brew services stop ollama 2>/dev/null
if [ $? -eq 0 ]; then
    echo "âœ… Ollama brew service stopped"
else
    echo "â„¹ï¸  Ollama brew service was not running"
fi

# Kill any remaining Ollama processes
echo "ğŸ”ª Killing any remaining Ollama processes..."
pkill -f "ollama"
if [ $? -eq 0 ]; then
    echo "âœ… Ollama processes killed"
else
    echo "â„¹ï¸  No Ollama processes found"
fi

# Optional: Free up GPU memory (if you were using GPU)
echo "ğŸ§¹ Clearing any cached models from memory..."
# This is more relevant for CUDA, but good practice
python3 -c "import gc; gc.collect()" 2>/dev/null

echo "=================================================="
echo "ğŸ‰ All LLM services have been shut down!"
echo "ğŸ’¾ Model files are still cached locally for faster startup next time"
echo "ğŸ”‹ Your system is now free to shut down safely"

# Check if any processes are still running
echo ""
echo "ğŸ” Checking for any remaining processes..."
REMAINING=$(ps aux | grep -E "(streamlit|ollama)" | grep -v grep | wc -l)
if [ $REMAINING -eq 0 ]; then
    echo "âœ… Clean shutdown - no processes remaining"
else
    echo "âš ï¸  Warning: $REMAINING processes may still be running:"
    ps aux | grep -E "(streamlit|ollama)" | grep -v grep
fi

# Storage cleanup option
echo ""
echo "=================================================="
echo "ğŸ’¾ Storage Cleanup (Optional)"
echo "=================================================="

# Function to get directory size
get_size() {
    if [ -d "$1" ]; then
        du -sh "$1" 2>/dev/null | cut -f1
    else
        echo "0B"
    fi
}

OLLAMA_SIZE=$(get_size ~/.ollama)
HF_SIZE=$(get_size ~/.cache/huggingface)

echo "ğŸ“Š Current Storage Usage:"
echo "ğŸ¦™ Ollama Models:      $OLLAMA_SIZE"
echo "ğŸ¤— Hugging Face Cache: $HF_SIZE"
echo ""

read -p "ğŸ§¹ Clean up model storage to free space? [y/N]: " cleanup_storage

if [[ $cleanup_storage =~ ^[Yy]$ ]]; then
    echo ""
    echo "ğŸ› ï¸  Storage Cleanup Options:"
    echo "1ï¸âƒ£  Keep models (just shutdown services)"
    echo "2ï¸âƒ£  Remove Ollama models (~$OLLAMA_SIZE)"
    echo "3ï¸âƒ£  Remove HuggingFace cache (~$HF_SIZE)" 
    echo "4ï¸âƒ£  Remove ALL models (frees ~7GB+)"
    echo ""
    
    read -p "Choose option [1-4]: " cleanup_option
    
    case $cleanup_option in
        2)
            echo "ğŸ—‘ï¸  Removing Ollama models..."
            ollama list --format json 2>/dev/null | jq -r '.[].name' | xargs -I {} ollama rm {} 2>/dev/null || echo "Manual cleanup required"
            rm -rf ~/.ollama/models/* 2>/dev/null
            echo "âœ… Ollama models removed"
            ;;
        3)
            echo "ğŸ—‘ï¸  Removing HuggingFace cache..."
            rm -rf ~/.cache/huggingface/*
            echo "âœ… HuggingFace cache cleared"
            ;;
        4)
            echo "ğŸ—‘ï¸  Removing ALL models..."
            ollama list --format json 2>/dev/null | jq -r '.[].name' | xargs -I {} ollama rm {} 2>/dev/null
            rm -rf ~/.ollama/models/* 2>/dev/null
            rm -rf ~/.cache/huggingface/*
            rm -rf ./results/*
            echo "âœ… All models and caches cleared"
            ;;
        *)
            echo "â„¹ï¸  Keeping all models"
            ;;
    esac
    
    echo ""
    echo "ğŸ“Š New Storage Usage:"
    echo "ğŸ¦™ Ollama Models:      $(get_size ~/.ollama)"
    echo "ğŸ¤— Hugging Face Cache: $(get_size ~/.cache/huggingface)"
fi

echo ""
echo "=================================================="
echo "ğŸ‰ Shutdown Complete!"
echo "ğŸ’¡ To restart later: ./start_services.sh"
echo "ğŸ’½ Models will auto-download when needed"
echo "==================================================" 