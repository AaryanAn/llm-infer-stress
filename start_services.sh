#!/bin/bash

echo "ğŸš€ Starting LLM Inference Stress Testing Services..."
echo "=================================================="

# Start Ollama service
echo "ğŸ¦™ Starting Ollama service..."
brew services start ollama
if [ $? -eq 0 ]; then
    echo "âœ… Ollama service started"
    # Wait for Ollama to be ready
    echo "â³ Waiting for Ollama to be ready..."
    sleep 3
    
    # Test if Ollama is responding
    if curl -s http://localhost:11434 > /dev/null; then
        echo "âœ… Ollama is responding on localhost:11434"
    else
        echo "âš ï¸  Ollama may need more time to start"
    fi
else
    echo "âŒ Failed to start Ollama service"
fi

# List available models
echo ""
echo "ğŸ“‹ Available Ollama models:"
ollama list 2>/dev/null || echo "âŒ Ollama not ready yet"

# Start Streamlit dashboard
echo ""
echo "ğŸ“± Starting Streamlit dashboard..."
echo "ğŸŒ Dashboard will be available at: http://localhost:8501"
echo "ğŸ“Š To start the dashboard, run:"
echo "   streamlit run app.py"

echo "=================================================="
echo "ğŸ‰ Services are ready!"
echo "ğŸ’¡ To test your models:"
echo "   1. Open http://localhost:8501 in your browser"
echo "   2. Select 'Ollama' as provider"
echo "   3. Choose your model and run stress tests"
echo ""
echo "ğŸ›‘ To shut down everything later, run:"
echo "   ./shutdown_services.sh" 